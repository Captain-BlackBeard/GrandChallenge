\documentclass{sig-alternate}

\usepackage{color}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{ifpdf}
\usepackage{graphicx}
\usepackage[table]{xcolor}
\usepackage{microtype}

\clubpenalty=10000
\widowpenalty=10000

\setlength{\paperheight}{11in}
\setlength{\paperwidth}{8.5in}
\usepackage[
pass,% keep layout unchanged 
% showframe,% show the layout
]{geometry}

\usepackage{listings}
\lstset{	basicstyle=\normalsize, 
			numbers=left, 
			xleftmargin=2em,
			frame=none,
			framexleftmargin=1.5em,
			numberstyle=\tiny, 
			tabsize=3, 
			language={Java}, 
			escapeinside={/*@}{@*/},
			showstringspaces=false,
			commentstyle=\color[cmyk]{0.3,0.3,0.3,0.3},
			stringstyle=\color[cmyk]{0.92,0,0.95,0.40},
			keywordstyle=\color{blue}\bfseries,
			showspaces=false, 
			showtabs=false}


\newcommand{\TITLE}{The DEBS 2016 Grand Challenge}
\newcommand{\KEYWORDS}{event processing, streaming, utilities, geo-spatial}
\newcommand{\VG}{Vincenzo Gulisano}
\newcommand{\VGEMAIL}{{\normalsize vincenzo.gulisano@chalmers.se}}
\newcommand{\ZJ}{Zbigniew Jerzak}
\newcommand{\ZJEMAIL}{{\normalsize zbigniew.jerzak@sap.com}}
\newcommand{\SV}{Spyros Voulgaris}
\newcommand{\SVEMAIL}{{\normalsize spyros@cs.vu.nl}}
\newcommand{\HZ}{Holger Ziekow}
\newcommand{\HZEMAIL}{{\normalsize zie@hs-furtwangen.de}}


\newcommand{\VGADDR}[1]{	\affaddr{Chalmers University of Technology}\\%
							\affaddr{H\"{o}rsalsv\"{a}gen 11}\\%	
							\affaddr{41296 Gothenburg, Sweden}\\%
							\email{#1}}							
\newcommand{\ZJADDR}[1]{	\affaddr{SAP SE}\\%
							\affaddr{M\"{u}nzstra{\ss}e 15}\\%
							\affaddr{10178 Berlin, Germany}\\%
							\email{#1}}
\newcommand{\SVADDR}[1]{	\affaddr{Vrije Universiteit Amsterdam}\\%
							\affaddr{De Boelelaan 1081A}\\%	
							\affaddr{1081HV Amsterdam, The Netherlands}\\%
							\email{#1}}							
\newcommand{\HZADDR}[1]{	\affaddr{Hochschule Furtwangen}\\%
							\affaddr{Robert-Gerwig-Platz 1}\\%	
							\affaddr{78120 Furtwangen, Germany}\\%
							\email{#1}}							

\ifpdf
	\usepackage[pdftex,%
					plainpages=false,%
					pdfpagelabels=false,
					bookmarksnumbered,%
					colorlinks=true,%
					linkcolor=blue,%
					citecolor=blue,%
					unicode=true]{hyperref} 
	\usepackage{pdfpages}
	\usepackage[all]{hypcap}
	\hypersetup{%
		pdftitle={\TITLE},
		pdfauthor={\VG, \ZJ, \SV, \HZ},
		pdfsubject={\TITLE},
		pdfkeywords={\KEYWORDS},
		bookmarksopen=false,
		unicode=true,
		colorlinks=true,
		hypertexnames=false}
\else
	\usepackage[dvipdfm,%
					pdftitle={\TITLE},
					pdfauthor={\VG, \ZJ, \SV, \HZ},
					pdfsubject={\TITLE},
					pdfkeywords={\KEYWORDS},
					bookmarks=true,%
					pdfpagelabels=false,%
					colorlinks=true,%
					linkcolor=blue,%
					citecolor=blue]{hyperref}
\fi

\begin{document}
	
  \newfont{\mycrnotice}{ptmr8t at 7pt}
  \newfont{\myconfname}{ptmri8t at 7pt}
  \let\crnotice\mycrnotice%
  \let\confname\myconfname%
  
  \permission{Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org.}
  \conferenceinfo{DEBS'15,} {June 29 -- July 3, 2015, OSLO, Norway.}
  \copyrightetc{Copyright 2015 ACM \the\acmcopyr}
  \crdata{978-1-4503-3286-6/15/06...\$15.00.\\ http://dx.doi.org/10.1145/2675743.2772598}

\title{\TITLE}

\numberofauthors{4}
\author{
\alignauthor	\VG\\
					\VGADDR{\VGEMAIL}
\alignauthor	\ZJ\\
					\ZJADDR{\ZJEMAIL}
\alignauthor	\SV\\
					\SVADDR{\SVEMAIL}
\and
\alignauthor	\HZ\\
					\HZADDR{\HZEMAIL}
}

\date{\today}
\maketitle

\begin{abstract}
Pending...
\end{abstract}


\category{C.2.4}{Computer-Communication Networks}{Distributed Systems}[Distributed Applications]
\terms{Algorithms, Design}
\keywords{\KEYWORDS} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Intro                       %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:introduction}
The ACM DEBS 2016 Grand Challenge is the sixth in a series~\cite{jerzak2012debs, mutschler2013debs, jerzak2014debs, jerzak2015debs} of challenges which seek to provide a common ground and uniform evaluation criteria for a competition aimed at both research and industrial event-based systems. The goal of the 2016 DEBS Grand Challenge competition is to evaluate event-based systems for real-time analytics over high volume data streams in the context of graph models.

The underlying scenario addresses the analysis metrics for a dynamic (evolving) social-network graph. Specifically, the 2016 Grand Challenge targets following problems: (1) identification of the posts that currently trigger the most activity in the social network, and (2) identification of large communities that are currently involved in a topic. The corresponding queries require continuous analysis of a dynamic graph under the consideration of multiple streams that reflect updates to the graph.

The data for the DEBS 2016 Grand Challenge is based on the dataset provided together with the LDBC Social Network Benchmark~\cite{erling2015social}. DEBS 2016 Grand Challenge takes up the general scenario from the 2014 SIGMOD Programming Contest~\cite{DBLP:conf/sigmod/2014}, however, in contrasts to the SIGMOD contest, it explicitly focuses on processing streaming data and thus dynamic graphs. Details about the data, queries for the Grand Challenge, and information about evaluation are provided below.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Data                       %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data}
\label{sec:data}
The data for the 2016 Grand Challenge is organized in four separate streams, each provided as a text file. The first input stream indicates when two users enter a "friendship" relationship -- see Table~\ref{table:friend} and Listing~\ref{code:friend}. The first input stream file name is $friendships.dat$.

\definecolor{lgray}{gray}{0.94}
\definecolor{llgray}{gray}{0.99}

\rowcolors{1}{lgray}{llgray}
\begin{table}[ht]
	\caption{The set of attributes used in the $friendships.dat$ input file}
	\centering 
	\begin{tabular}{r p{5.2cm}}
		\toprule
		Attribute		&	 Description\\
		\midrule
		ts			&	timestamp indicating when a friendship was established\\[2ex]
		user\_id\_1	&	id of one of the users\\[2ex]
		user\_id\_2	&	id of the other user\\[2ex]		
		\bottomrule 
	\end{tabular}
	\label{table:friend}
\end{table}



\lstset{}
\begin{lstlisting}[float=ht,caption={First line from the $friendships.dat$ file -- one attribute per line of listing},label={code:friend}]
XXX
YYY
ZZZ
PLEASE ADD ACTUAL DATA HERE
\end{lstlisting}	

The second input stream indicates when a users creates a new post -- see Table~\ref{table:post} and Listing~\ref{code:post}. The second input stream file name is $posts.dat$.

\rowcolors{1}{lgray}{llgray}
\begin{table}[ht]
	\caption{The set of attributes used in the $posts.dat$ input file}
	\centering 
	\begin{tabular}{r p{5.2cm}}
		\toprule
		Attribute		&	 Description\\
		\midrule
		ts			&	timestamp indicating when a post was created\\[2ex]
		post\_id	&	unique id of the post\\[2ex]
		user\_id	&	unique id of the user who created the post\\[2ex]		
		post		& 	string containing the post's content\\[2ex]		
		user		&   string containing the user name of the post creator\\[2ex]
		\bottomrule 
	\end{tabular}
	\label{table:post}
\end{table}



\lstset{}
\begin{lstlisting}[float=ht,caption={First line from the $posts.dat$ file -- one attribute per line of listing},label={code:post}]
XXX
YYY
ZZZ
PLEASE ADD ACTUAL DATA HERE
\end{lstlisting}

The third input stream indicates when a users commnets on a post -- see Table~\ref{table:comment} and Listing~\ref{code:comment}. The third input stream file name is $comments.dat$.

\rowcolors{1}{lgray}{llgray}
\begin{table}[ht]
	\caption{The set of attributes used in the $comments.dat$ input file}
	\centering 
	\begin{tabular}{r p{5.2cm}}
		\toprule
		Attribute		&	 Description\\
		\midrule
		ts			&	timestamp indicating when a comment was created\\[2ex]
		comment\_id	&	unique id of the comment\\[2ex]
		user\_id	&	unique id of the user who created the comment\\[2ex]		
		comment		& 	string containing the comment's content\\[2ex]		
		user		&   string containing the user name of the comment creator\\[2ex]
		comment\_replied		&   id of the comment being commented (-1 if this is a comment to a post)\\[2ex]
		post\_commented		&   id of the post being commented (-1 if this is a comment to a comment)\\[2ex]
		\bottomrule 
	\end{tabular}
	\label{table:comment}
\end{table}



\lstset{}
\begin{lstlisting}[float=ht,caption={First line from the $comments.dat$ file -- one attribute per line of listing},label={code:comment}]
XXX
YYY
ZZZ
PLEASE ADD ACTUAL DATA HERE
\end{lstlisting}

The fourth input stream indicates when a users likes a comment -- see Table~\ref{table:like} and Listing~\ref{code:like}. The fourth input stream file name is $likes.dat$.

\rowcolors{1}{lgray}{llgray}
\begin{table}[ht]
	\caption{The set of attributes used in the $likes.dat$ input file}
	\centering 
	\begin{tabular}{r p{5.2cm}}
		\toprule
		Attribute		&	 Description\\
		\midrule
		ts			&	timestamp indicating when user liked a comment\\[2ex]
		user\_id	&	unique id of the user who liked the comment\\[2ex]		
		comment\_id	& 	unique id of the comment that was liked\\[2ex]		
		\bottomrule 
	\end{tabular}
	\label{table:like}
\end{table}



\lstset{}
\begin{lstlisting}[float=ht,caption={First line from the $comments.dat$ file -- one attribute per line of listing},label={code:like}]
XXX
YYY
ZZZ
PLEASE ADD ACTUAL DATA HERE
\end{lstlisting}

Each of the data files is sorted chronologically based on the timestamp ($ts$) attribute.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Query 1                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Three Top Scoring Posts Query}
\label{sec:query1}
The goal of the first query is to compute the top three scoring active posts, producing an updated result every time the list changes. The total score of an active post $P$ is computed as the sum of its own score plus the score of all its related comments. Active posts having the same total score should be ranked based on their timestamps -- in descending order. And if their timestamps are also identical, they should be ranked based on the timestamps of their last received related comments -- in descending order. A comment $C$ is related to a post $P$ if it is a direct reply to $P$ or if the chain of $C$'s preceding messages links back to $P$.

Each new post has an initial own score of 10 which decreases by 1 each time another 24 hours elapse since the post's creation. Each new comment's score is also initially set to 10 and decreases by 1 in the same way, i.e., every 24 hours since the comment's creation. Both post and comment scores are non-negative numbers, that is, they cannot drop below zero. A post is considered no longer active (that is, no longer part of the present and future analysis) as soon as its total score reaches zero, even if it receives additional comments in the future. The output format for the result stream is shown in Table~\ref{table:query1}. 

\rowcolors{1}{lgray}{llgray}
\begin{table}[ht]
	\caption{The set of attributes used in the $likes.dat$ input file}
	\centering 
	\begin{tabular}{r p{5.2cm}}
		\toprule
		Attribute		&	 Description\\
		\midrule
		ts			&	the timestamp of the tuple event that triggers a change in the top-3 scoring active posts appearing in the rest of the tuple\\[2ex]
		topX\_post\_id	&	the unique id of the top-X post\\[2ex]		
		topX\_post\_user	& 	 author of top-X post\\[2ex]		
		topX\_post\_commenters & number of unique users commenting on the top-X post, excluding the post author\\[2ex]
		\bottomrule 
	\end{tabular}
	\label{table:query1}
\end{table}

Results must be sorted by their timestamp ($ts$) field. The character ,,--'' (a minus sign without the quotation marks) should be used for each of the fields (topX\_post\_id, topX\_post\_user, topX\_post\_commenters) of any of the top three positions that has not been defined. The logical time of the query advances based on the timestamps of the input tuples, not the system clock. Listing~\ref{code:query1} shows the example output for the first query. 

\begin{lstlisting}[float=ht,caption={Output example for the three top scoring posts query},label={code:query1}]
2010-09-19 12:33:01.923+0000,/*@\\@*/  25769805561,Karl Fischer,115,10,/*@\\@*/  25769805933,Chong Liu,83,4,/*@\\@*/  -,-,-,-
2010-10-09 21:55:24.943+0000,/*@\\@*/  34359739095,Karl Fischer,58,7,/*@\\@*/  34359740594,Paul Becker,40,2,/*@\\@*/  34359740220,Chong Zhang,10,0
2010-12-27 22:11:54.953+0000,/*@\\@*/  42949673675,Anson Chen,127,12,/*@\\@*/  42949673684,Yahya Abdallahi,69,8,/*@\\@*/  42949674571,Alim Guliyev,10,0
\end{lstlisting}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Query 2                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Community Interest Change Query}
The goal of profitable areas query is to identify areas that are currently most profitable for taxi drivers. The profitability of an area is determined by dividing the area profit by the number of empty taxis in that area within the last 15 minutes. The profit that originates from an area is computed by calculating the median fare including tip for trips that started in the area and ended within the last 15 minutes. The number of empty taxis in an area is the sum of taxis that had a drop-off location in that area less than 30 minutes ago and had no following pickup yet.

The result stream of the query must list the ten most profitable areas in the format presented in Listing~\ref{code:query2}.

\begin{lstlisting}[float=ht,caption={Output format for the profitable areas query},label={code:query2}]
pickup_datetime
dropoff_datetime
profitable_cell_id_1
empty_taxies_in_cell_id_1
median_profit_in_cell_id_1
profitability_of_cell_1
... 
profitable_cell_id_10
empty_taxies_in_cell_id_10
median_profit_in_cell_id_10
profitability_of_cell_10
delay
\end{lstlisting}

with attribute names containing cell\_id\_1 corresponding to the most profitable cell and attribute containing cell\_id\_10 corresponding to the 10$^{th}$ most profitable cell. If less than 10 cells were identified within the last 30 minutes, then NULL is to be returned for all cells that lack data. Query results must be updated whenever any of the 10 most profitable areas change. The pickup\_datetime and dropoff\_datetime in the output are the timestamps of the trip report that triggered the change.

The attribute "delay" captures the time delay between reading the input event that triggered the output and the time when the output is produced. Participants must determine the delay using the current system time right after reading the input and right before writing the output, i.e., including the serialization and deserialization time but excluding the disk IO time. 

Profitable areas query uses the same numbering scheme as for frequent routes query, however it uses a different resolution. In this query one should assume a cell size of 250m X 250m, i.e., the area to be considered spans from cell 1.1 to cell 600.600.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Additional Remarks          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Additional Remarks}
For both queries it is assumed that the result is calculated in a streaming fashion - i.e.: (1) solutions must not make use of any pre-calculated information, such as indices and (2) result streams must be updated continuously.

The ranking of the submissions is done using the total duration of the execution and the average delay per stream. Specifically, solutions are ranked (a) by the total execution time as well as (b) by the average latency. The final result is calculated as a sum of average results from both query result streams. The average latency should be computed as the average of each output tuple latency, the latter measured as the difference between the system clock taken when logging an output tuple and the system clock taken when the processing of the input tuple triggering the result starts. The final ranking is determined by a score that is the sum of the rank for execution time and the rank for delay. For solutions with the same overall score the solution with the lower execution time will be ranked higher.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% License                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{License}
All solutions submitted to the DEBS 2016 Grand Challenge are open source under the BSD license: \url{https://opensource.org/licenses/BSD-3-Clause}. A solution incorporates concepts, queries, and code developed for the purpose of solving the Grand Challenge. If a solution is developed within the context of, is built on top of, or is using an existing system or solution which is licensed under a different license than BSD, then such an existing solution or system maintains its existing license.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Acknowledgements            %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Acknowledgements}
The DEBS Grand Challenge Organizing Committee would like to explicitly thank WSO2 (\url{http://wso2.com}) for sponsoring the DEBS 2016 Grand Challenge prize and the LDBC Council (\url{http://www.ldbcouncil.org}) for their help in preparing the test data set.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% References                  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{plain}
\bibliography{debsgc2016}

%\balancecolumns

\end{document}
